# -*- coding: utf-8 -*-
"""Emoji_Predictor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HON5PSr1zKOv6lDrBpkNKeFqIqlp8z9H

##***Get the Emoji Package***
"""

pip install emoji

import emoji

emoji.EMOJI_UNICODE

emoji_dict= {
        '0' : '\u2764\uFE0F',
        '1' : ':baseball:',
        '2' : ':grinning_face_with_big_eyes:',
        '3' : ':disappointed_face:',
        '4' : ':fork_and_knife:'
}

for i in emoji_dict.values():
    print(emoji.emojize(i))

"""##Processing a Custom dataset"""

import pandas as pd
import numpy as np

train = pd.read_csv('train_emoji.csv',header=None)
test = pd.read_csv('test_emoji.csv',header=None)

x_train= train.iloc[:,0].values
x_test= test.iloc[:,0].values

y_test = test.iloc[:,1].values
y_train= train.iloc[:,1].values

for i in range(5):
    print(x_train[i],emoji.emojize(emoji_dict[str(y_train[i])]))

from keras.utils import to_categorical
y_train = to_categorical(y_train,num_classes=5)
y_test = to_categorical(y_test,num_classes=5)

"""## Embedding the Sentences"""

f = open('glove.6B.50d.txt',encoding='utf-8')

gloVe_coeff={}
c=1
for line in f:
    values= line.split()
    word = values[0]
    coeff = np.asarray(values[1:],dtype='float')
    gloVe_coeff[word]= coeff
f.close()

gloVe_coeff['eat'].shape

"""##Converting Sentences into Vector"""

def embedding_output(X):
    maxLen=10
    emb_dim= 50
    embedding_out = np.zeros((X.shape[0],maxLen,emb_dim))
    for i in range(X.shape[0]):
        X[i] = X[i].split()
        for j in range(len(X[i])):
            try:
                embedding_out[i][j] = gloVe_coeff[X[i][j].lower()]
            except:
                embedding_out[i][j] = np.zeros((emb_dim,))
    return embedding_out



embedding_matrix_train = embedding_output(x_train)
embedding_matrix_test = embedding_output(x_test)

print(embedding_matrix_test.shape)
print(embedding_matrix_train.shape)

"""## Building the LSTM"""

from keras.models import Sequential
from keras.layers import *

classifier =  Sequential()
classifier.add(LSTM(64,input_shape=(10,50),return_sequences=True))
classifier.add(Dropout(rate=0.5))

classifier.add(LSTM(64,input_shape=(10,50)))
classifier.add(Dense(5))
classifier.add(Activation('softmax'))
classifier.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
classifier.summary()

"""##Training the model"""

from keras.callbacks import EarlyStopping,ModelCheckpoint

checkpoint = ModelCheckpoint('best_model.h5',monitor='val_loss',verbose=True,save_best_only=True)
stop = EarlyStopping(monitor='val_acc',patience=20)

hist=classifier.fit(embedding_matrix_train,y_train,epochs=200,validation_split=0.2,batch_size=64,shuffle=True,callbacks=[checkpoint,stop])

classifier.load_weights('best_model.h5')

classifier.evaluate(embedding_matrix_test,y_test)

pred=classifier.predict_classes(embedding_matrix_test)

for i in range(30):
    print(' '.join(x_test[i]))
    print(emoji.emojize(emoji_dict[str(np.argmax(y_test[i]))]))
    print(emoji.emojize(emoji_dict[str(pred[i])]))